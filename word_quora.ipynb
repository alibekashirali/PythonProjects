{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель — научиться вычислять эмбеддинги текстов.\n",
    "Необходимо сделать модель машинного обучения, вычисляющую эмбеддинги текста (векторное представление фиксированной длины), используя готовые наработки либо обучив её самостоятельно. Вид векторного представления может быть произвольным, но каждый из эмбеддингов не должен содержать более 1000 чисел.\n",
    "\n",
    "После чего нужно по датасету вопросов Quora Question Pairs (https://www.kaggle.com/c/quora-question-pairs/) обучить модель, возвращающую вероятность похожести пар вопросов только по паре их эмбеддингов. Модель должна выдавать на тестовой выборке в публичном лидерборде kaggle logloss не больше 0,5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Вычисление эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использую готовую библиотеку для вычисления эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alibekk\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('clegg', 0.9653651118278503),\n",
       " ('miliband', 0.9515050053596497),\n",
       " ('bachmann', 0.9484400749206543),\n",
       " ('mcconnell', 0.9416399002075195),\n",
       " ('carney', 0.934025764465332),\n",
       " ('coulter', 0.9311323761940002),\n",
       " ('boehner', 0.9286302328109741),\n",
       " ('santorum', 0.9269059300422668),\n",
       " ('farage', 0.919365406036377),\n",
       " ('mourdock', 0.9186689853668213)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"pelosi\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Не стал запускать код - долго обучает, по времени не успел()\n",
    "corpus = api.load('text8')\n",
    "\n",
    "# обучение модели word2vec на корпусе текстов\n",
    "model_text8 = Word2Vec(corpus,iter=10, size=1000, window=10, min_count=2, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Quora Question Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "test_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_train = train_qs.apply(lambda x: len(x.split(' ')))\n",
    "dist_test = test_qs.apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление стопслов и начальный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alibekk\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "tfidf_train_word_match = df_train.apply(tfidf_word_match_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original AUC: 0.7804327049353577\n",
      " TFIDF AUC: 0.7704802292218704\n"
     ]
    }
   ],
   "source": [
    "print('Original AUC:', roc_auc_score(df_train['is_duplicate'], train_word_match))\n",
    "print(' TFIDF AUC:', roc_auc_score(df_train['is_duplicate'], tfidf_train_word_match.fillna(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балансировка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alibekk\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Alibekk\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.DataFrame()\n",
    "x_test = pd.DataFrame()\n",
    "x_train['word_match'] = train_word_match\n",
    "x_train['tfidf_word_match'] = tfidf_train_word_match\n",
    "x_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\n",
    "x_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "\n",
    "y_train = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19124366100096607\n"
     ]
    }
   ],
   "source": [
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]\n",
    "\n",
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=1242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применяю модель xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.682797\tvalid-logloss:0.683336\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.602389\tvalid-logloss:0.602324\n",
      "[20]\ttrain-logloss:0.545213\tvalid-logloss:0.545464\n",
      "[30]\ttrain-logloss:0.503348\tvalid-logloss:0.503943\n",
      "[40]\ttrain-logloss:0.471667\tvalid-logloss:0.472932\n",
      "[50]\ttrain-logloss:0.448513\tvalid-logloss:0.449326\n",
      "[60]\ttrain-logloss:0.430594\tvalid-logloss:0.431156\n",
      "[70]\ttrain-logloss:0.416344\tvalid-logloss:0.41716\n",
      "[80]\ttrain-logloss:0.405347\tvalid-logloss:0.40619\n",
      "[90]\ttrain-logloss:0.396571\tvalid-logloss:0.397536\n",
      "[100]\ttrain-logloss:0.389763\tvalid-logloss:0.39076\n",
      "[110]\ttrain-logloss:0.38428\tvalid-logloss:0.385381\n",
      "[120]\ttrain-logloss:0.379937\tvalid-logloss:0.381106\n",
      "[130]\ttrain-logloss:0.376644\tvalid-logloss:0.37769\n",
      "[140]\ttrain-logloss:0.374052\tvalid-logloss:0.37504\n",
      "[150]\ttrain-logloss:0.371656\tvalid-logloss:0.372877\n",
      "[160]\ttrain-logloss:0.369929\tvalid-logloss:0.371055\n",
      "[170]\ttrain-logloss:0.368575\tvalid-logloss:0.369652\n",
      "[180]\ttrain-logloss:0.367545\tvalid-logloss:0.368578\n",
      "[190]\ttrain-logloss:0.366621\tvalid-logloss:0.36771\n",
      "[200]\ttrain-logloss:0.365808\tvalid-logloss:0.367\n",
      "[210]\ttrain-logloss:0.365088\tvalid-logloss:0.366368\n",
      "[220]\ttrain-logloss:0.364726\tvalid-logloss:0.365921\n",
      "[230]\ttrain-logloss:0.364263\tvalid-logloss:0.365494\n",
      "[240]\ttrain-logloss:0.363753\tvalid-logloss:0.365127\n",
      "[250]\ttrain-logloss:0.363646\tvalid-logloss:0.364829\n",
      "[260]\ttrain-logloss:0.363497\tvalid-logloss:0.36458\n",
      "[270]\ttrain-logloss:0.363129\tvalid-logloss:0.36436\n",
      "[280]\ttrain-logloss:0.362944\tvalid-logloss:0.364164\n",
      "[290]\ttrain-logloss:0.362802\tvalid-logloss:0.363904\n",
      "[300]\ttrain-logloss:0.362458\tvalid-logloss:0.36379\n",
      "[310]\ttrain-logloss:0.362342\tvalid-logloss:0.363631\n",
      "[320]\ttrain-logloss:0.362242\tvalid-logloss:0.363511\n",
      "[330]\ttrain-logloss:0.362076\tvalid-logloss:0.363349\n",
      "[340]\ttrain-logloss:0.362002\tvalid-logloss:0.363201\n",
      "[350]\ttrain-logloss:0.361897\tvalid-logloss:0.363091\n",
      "[360]\ttrain-logloss:0.36178\tvalid-logloss:0.362978\n",
      "[370]\ttrain-logloss:0.361507\tvalid-logloss:0.362872\n",
      "[380]\ttrain-logloss:0.361433\tvalid-logloss:0.36279\n",
      "[390]\ttrain-logloss:0.361377\tvalid-logloss:0.362727\n",
      "[400]\ttrain-logloss:0.361275\tvalid-logloss:0.362671\n",
      "[410]\ttrain-logloss:0.361231\tvalid-logloss:0.362572\n",
      "[420]\ttrain-logloss:0.361105\tvalid-logloss:0.362481\n",
      "[430]\ttrain-logloss:0.360993\tvalid-logloss:0.362368\n",
      "[440]\ttrain-logloss:0.360859\tvalid-logloss:0.362276\n",
      "[450]\ttrain-logloss:0.360753\tvalid-logloss:0.362185\n",
      "[460]\ttrain-logloss:0.36066\tvalid-logloss:0.362096\n",
      "[470]\ttrain-logloss:0.360567\tvalid-logloss:0.361999\n",
      "[480]\ttrain-logloss:0.360477\tvalid-logloss:0.361923\n",
      "[490]\ttrain-logloss:0.360405\tvalid-logloss:0.361863\n",
      "[499]\ttrain-logloss:0.360354\tvalid-logloss:0.361817\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 500, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
